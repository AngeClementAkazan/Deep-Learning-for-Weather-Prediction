{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ange-clementakazan/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from  Kan_Layer import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError, R2Score,MeanSquaredError,MeanAbsoluteError\n",
    "np.random.seed(142)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/ct8x_h9j78xbj8gnyb35c3_w0000gn/T/ipykernel_40168/1676934423.py:1: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  Abj_dt = pd.read_csv('/Users/ange-clementakazan/Documents/Weather_Prediction_project/Deep-Learning-for-Weather-Prediction/data/Abidjan_data.csv',parse_dates={'datetime': ['YEAR', 'MO','DY']})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASWD</th>\n",
       "      <th>CSWD</th>\n",
       "      <th>ALWD</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2MDEW</th>\n",
       "      <th>T2MWET</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>RH2M</th>\n",
       "      <th>PS</th>\n",
       "      <th>PREC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>4.06</td>\n",
       "      <td>5.86</td>\n",
       "      <td>412.57</td>\n",
       "      <td>28.05</td>\n",
       "      <td>22.73</td>\n",
       "      <td>25.39</td>\n",
       "      <td>17.33</td>\n",
       "      <td>74.56</td>\n",
       "      <td>100.44</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>5.21</td>\n",
       "      <td>5.58</td>\n",
       "      <td>408.32</td>\n",
       "      <td>28.34</td>\n",
       "      <td>22.48</td>\n",
       "      <td>25.41</td>\n",
       "      <td>17.09</td>\n",
       "      <td>72.56</td>\n",
       "      <td>100.45</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>4.77</td>\n",
       "      <td>5.38</td>\n",
       "      <td>412.82</td>\n",
       "      <td>28.69</td>\n",
       "      <td>22.51</td>\n",
       "      <td>25.60</td>\n",
       "      <td>17.15</td>\n",
       "      <td>71.75</td>\n",
       "      <td>100.41</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>5.17</td>\n",
       "      <td>5.55</td>\n",
       "      <td>407.15</td>\n",
       "      <td>28.44</td>\n",
       "      <td>22.46</td>\n",
       "      <td>25.45</td>\n",
       "      <td>17.15</td>\n",
       "      <td>73.00</td>\n",
       "      <td>100.43</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>5.26</td>\n",
       "      <td>5.57</td>\n",
       "      <td>406.60</td>\n",
       "      <td>28.20</td>\n",
       "      <td>22.46</td>\n",
       "      <td>25.33</td>\n",
       "      <td>17.09</td>\n",
       "      <td>73.50</td>\n",
       "      <td>100.43</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>4.47</td>\n",
       "      <td>5.38</td>\n",
       "      <td>401.95</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.82</td>\n",
       "      <td>25.09</td>\n",
       "      <td>17.46</td>\n",
       "      <td>77.75</td>\n",
       "      <td>100.65</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>4.44</td>\n",
       "      <td>5.26</td>\n",
       "      <td>410.70</td>\n",
       "      <td>27.99</td>\n",
       "      <td>24.05</td>\n",
       "      <td>26.01</td>\n",
       "      <td>18.74</td>\n",
       "      <td>80.06</td>\n",
       "      <td>100.55</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>5.12</td>\n",
       "      <td>5.40</td>\n",
       "      <td>409.38</td>\n",
       "      <td>27.72</td>\n",
       "      <td>24.11</td>\n",
       "      <td>25.91</td>\n",
       "      <td>18.80</td>\n",
       "      <td>81.94</td>\n",
       "      <td>100.53</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>4.95</td>\n",
       "      <td>5.33</td>\n",
       "      <td>406.38</td>\n",
       "      <td>27.71</td>\n",
       "      <td>23.76</td>\n",
       "      <td>25.74</td>\n",
       "      <td>18.43</td>\n",
       "      <td>80.31</td>\n",
       "      <td>100.53</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>4.96</td>\n",
       "      <td>5.33</td>\n",
       "      <td>410.02</td>\n",
       "      <td>27.53</td>\n",
       "      <td>24.08</td>\n",
       "      <td>25.80</td>\n",
       "      <td>18.74</td>\n",
       "      <td>82.38</td>\n",
       "      <td>100.58</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5114 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ASWD  CSWD    ALWD    T2M  T2MDEW  T2MWET   QV2M   RH2M      PS  \\\n",
       "datetime                                                                      \n",
       "2010-01-01  4.06  5.86  412.57  28.05   22.73   25.39  17.33  74.56  100.44   \n",
       "2010-01-02  5.21  5.58  408.32  28.34   22.48   25.41  17.09  72.56  100.45   \n",
       "2010-01-03  4.77  5.38  412.82  28.69   22.51   25.60  17.15  71.75  100.41   \n",
       "2010-01-04  5.17  5.55  407.15  28.44   22.46   25.45  17.15  73.00  100.43   \n",
       "2010-01-05  5.26  5.57  406.60  28.20   22.46   25.33  17.09  73.50  100.43   \n",
       "...          ...   ...     ...    ...     ...     ...    ...    ...     ...   \n",
       "2023-12-28  4.47  5.38  401.95  27.37   22.82   25.09  17.46  77.75  100.65   \n",
       "2023-12-29  4.44  5.26  410.70  27.99   24.05   26.01  18.74  80.06  100.55   \n",
       "2023-12-30  5.12  5.40  409.38  27.72   24.11   25.91  18.80  81.94  100.53   \n",
       "2023-12-31  4.95  5.33  406.38  27.71   23.76   25.74  18.43  80.31  100.53   \n",
       "2024-01-01  4.96  5.33  410.02  27.53   24.08   25.80  18.74  82.38  100.58   \n",
       "\n",
       "            PREC  \n",
       "datetime          \n",
       "2010-01-01  0.08  \n",
       "2010-01-02  0.05  \n",
       "2010-01-03  0.08  \n",
       "2010-01-04  0.05  \n",
       "2010-01-05  0.06  \n",
       "...          ...  \n",
       "2023-12-28  0.15  \n",
       "2023-12-29  0.17  \n",
       "2023-12-30  0.26  \n",
       "2023-12-31  0.27  \n",
       "2024-01-01  2.01  \n",
       "\n",
       "[5114 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abj_dt = pd.read_csv('/Users/ange-clementakazan/Documents/Weather_Prediction_project/Deep-Learning-for-Weather-Prediction/data/Abidjan_data.csv',parse_dates={'datetime': ['YEAR', 'MO','DY']})\n",
    "# Abj_dt\n",
    "#Rename Columns\n",
    "col_name_map = {\n",
    "    \"ALLSKY_SFC_SW_DWN\": \"ASWD\",\n",
    "    \"CLRSKY_SFC_SW_DWN\":\"CSWD\",\"ALLSKY_SFC_LW_DWN\":\"ALWD\",\"PRECTOTCORR\":\"PREC\"\n",
    "}\n",
    "# new_colnames=['datetime', 'ASWD', 'CSWD',\n",
    "#        'ALWD', 'T2M', 'T2MDEW', 'T2MWET', 'QV2M', 'RH2M',\n",
    "#        'PREC', 'PS']\n",
    "# Rename the columns for both datasets\n",
    "Abj_dt=Abj_dt.set_index(\"datetime\")\n",
    "Abj_dt=Abj_dt.rename(columns=col_name_map)\n",
    "# Xy_train\n",
    "# target_col = Abj_dt.pop(\"T2M\")\n",
    "# Abj_dt.insert(len(Abj_dt.columns),\"T2M\", target_col)\n",
    "# Abj_dt\n",
    "target_col = Abj_dt.pop(\"PREC\")\n",
    "Abj_dt.insert(len(Abj_dt.columns),\"PREC\", target_col)\n",
    "Abj_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TKANCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        inputs,\n",
    "        sub_kan_configs=None,\n",
    "        sub_kan_output_dim=None,\n",
    "        sub_kan_input_dim=None,\n",
    "        activation=torch.tanh,\n",
    "        recurrent_activation=torch.sigmoid,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=nn.init.xavier_uniform_,\n",
    "        recurrent_initializer=nn.init.orthogonal_,\n",
    "        bias_initializer=nn.init.zeros_,\n",
    "        unit_forget_bias=True,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        seed=None\n",
    "    ):\n",
    "        super(TKANCell, self).__init__()\n",
    "        self.units = units\n",
    "        self.inputs=inputs\n",
    "        self.sub_kan_configs = sub_kan_configs or [None]\n",
    "        self.sub_kan_output_dim = sub_kan_output_dim\n",
    "        self.sub_kan_input_dim = sub_kan_input_dim\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.seed = seed if seed is not None else int(time.time())\n",
    "\n",
    "        self.kernel = nn.Parameter(torch.Tensor(units * 3, sub_kan_input_dim))\n",
    "        self.recurrent_kernel = nn.Parameter(torch.Tensor(units * 3, units))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(units * 3))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        # Initializing sub-KAN layers\n",
    "        self.tkan_sub_layers = nn.ModuleList()\n",
    "        for config in self.sub_kan_configs:\n",
    "        #     if config is None:\n",
    "                # layer = KANLinear(self.sub_kan_output_dim, use_layernorm=True)\n",
    "            layer_method=KANLinear(self.sub_kan_input_dim,self.sub_kan_output_dim)\n",
    "            layer=layer_method(self.inputs)\n",
    "\n",
    "            # elif isinstance(config, (int, float)):\n",
    "            #     layer = KANLinear(self.sub_kan_output_dim, spline_order=config, use_layernorm=True)\n",
    "            # elif isinstance(config, dict):\n",
    "            #     layer = KANLinear(self.sub_kan_output_dim, **config, use_layernorm=True)\n",
    "            # else:\n",
    "            #     layer = nn.Linear(self.sub_kan_input_dim, self.sub_kan_output_dim)\n",
    "            self.tkan_sub_layers.append(layer)\n",
    "\n",
    "        self.sub_tkan_kernel = nn.Parameter(torch.Tensor(len(self.tkan_sub_layers), self.sub_kan_output_dim * 2))\n",
    "        self.sub_tkan_recurrent_kernel_inputs = nn.Parameter(\n",
    "            torch.Tensor(len(self.tkan_sub_layers), sub_kan_input_dim, self.sub_kan_input_dim))\n",
    "        self.sub_tkan_recurrent_kernel_states = nn.Parameter(\n",
    "            torch.Tensor(len(self.tkan_sub_layers), self.sub_kan_output_dim, self.sub_kan_input_dim))\n",
    "\n",
    "        self.aggregated_weight = nn.Parameter(torch.Tensor(len(self.tkan_sub_layers) * self.sub_kan_output_dim, self.units))\n",
    "        self.aggregated_bias = nn.Parameter(torch.Tensor(self.units))\n",
    "\n",
    "        # Parameter initialization\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.kernel_initializer(self.kernel)\n",
    "        self.recurrent_initializer(self.recurrent_kernel)\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                self.bias.data[:self.units] = 0\n",
    "                self.bias.data[self.units:2*self.units] = 1\n",
    "                self.bias.data[2*self.units:] = 0\n",
    "            else:\n",
    "                self.bias_initializer(self.bias)\n",
    "        nn.init.xavier_uniform_(self.sub_tkan_kernel)\n",
    "        nn.init.xavier_uniform_(self.sub_tkan_recurrent_kernel_inputs)\n",
    "        nn.init.xavier_uniform_(self.sub_tkan_recurrent_kernel_states)\n",
    "        nn.init.xavier_uniform_(self.aggregated_weight)\n",
    "        nn.init.zeros_(self.aggregated_bias)\n",
    "\n",
    "    def forward(self,  states):\n",
    "        h_tm1, c_tm1 = states[:2]  # Previous hidden state and cell state\n",
    "        sub_states = states[2:]  # Sub-layer states\n",
    "        inputs_=self.inputs\n",
    "        if self.training and self.dropout > 0.0:\n",
    "            inputs_ = F.dropout(self.inputs, self.dropout, training=True)\n",
    "            h_tm1 = F.dropout(h_tm1, self.recurrent_dropout, training=True)\n",
    "\n",
    "        gates = F.linear(inputs_, self.kernel) + F.linear(h_tm1, self.recurrent_kernel)\n",
    "        if self.use_bias:\n",
    "            gates += self.bias\n",
    "\n",
    "        i, f, c = gates.chunk(3, dim=-1)\n",
    "        i, f = self.recurrent_activation(i), self.recurrent_activation(f)\n",
    "        c = f * c_tm1 + i * self.activation(c)\n",
    "\n",
    "        sub_outputs, new_sub_states = [], []\n",
    "        for idx, (sub_layer, sub_state) in enumerate(zip(self.tkan_sub_layers, sub_states)):\n",
    "            agg_input = F.linear(inputs_, self.sub_tkan_recurrent_kernel_inputs[idx]) + \\\n",
    "                        F.linear(sub_state, self.sub_tkan_recurrent_kernel_states[idx])\n",
    "            sub_output = sub_layer(agg_input)\n",
    "            sub_recurrent_kernel_h, sub_recurrent_kernel_x = self.sub_tkan_kernel[idx].chunk(2, dim=0)\n",
    "            new_sub_state = sub_recurrent_kernel_h * sub_output + sub_recurrent_kernel_x * sub_state\n",
    "\n",
    "            sub_outputs.append(sub_output)\n",
    "            new_sub_states.append(new_sub_state)\n",
    "\n",
    "        aggregated_sub_output = torch.cat(sub_outputs, dim=-1)\n",
    "        aggregated_input = F.linear(aggregated_sub_output, self.aggregated_weight) + self.aggregated_bias\n",
    "        o = self.recurrent_activation(aggregated_input)\n",
    "        h = o * self.activation(c)\n",
    "\n",
    "        return h, [h, c] + new_sub_states\n",
    "\n",
    "\n",
    "class TKAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        inputs,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(TKAN, self).__init__()\n",
    "        self.cell = TKANCell(units,inputs, **kwargs)\n",
    "\n",
    "    def forward(self, sequences, initial_state=None):\n",
    "        batch_size, timesteps, _ = sequences.shape\n",
    "\n",
    "        if initial_state is None:\n",
    "            initial_state = [torch.zeros((batch_size, self.cell.units), device=sequences.device)] * (2 + len(self.cell.tkan_sub_layers))\n",
    "\n",
    "        outputs, state = [], initial_state\n",
    "        for t in range(timesteps):\n",
    "            output, state = self.cell(sequences[:, t, :], state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Abj_dt = pd.read_csv('/Users/ange-clementakazan/Documents/Weather_Prediction_project/Deep-Learning-for-Weather-Prediction/data/Abidjan_data.csv',parse_dates={'datetime': ['YEAR', 'MO','DY']})\n",
    "# Abj_dt\n",
    "#Rename Columns\n",
    "col_name_map = {\n",
    "    \"ALLSKY_SFC_SW_DWN\": \"ASWD\",\n",
    "    \"CLRSKY_SFC_SW_DWN\":\"CSWD\",\"ALLSKY_SFC_LW_DWN\":\"ALWD\",\"PRECTOTCORR\":\"PREC\"\n",
    "}\n",
    "# new_colnames=['datetime', 'ASWD', 'CSWD',\n",
    "#        'ALWD', 'T2M', 'T2MDEW', 'T2MWET', 'QV2M', 'RH2M',\n",
    "#        'PREC', 'PS']\n",
    "# Rename the columns for both datasets\n",
    "Abj_dt=Abj_dt.set_index(\"datetime\")\n",
    "Abj_dt=Abj_dt.rename(columns=col_name_map)\n",
    "# Xy_train\n",
    "# target_col = Abj_dt.pop(\"T2M\")\n",
    "# Abj_dt.insert(len(Abj_dt.columns),\"T2M\", target_col)\n",
    "# Abj_dt\n",
    "target_col = Abj_dt.pop(\"PREC\")\n",
    "Abj_dt.insert(len(Abj_dt.columns),\"PREC\", target_col)\n",
    "Abj_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "def Make_tidydata(data,scaler):\n",
    "    data= data.sort_index()\n",
    "    data[data.columns] = scaler.fit_transform(data[data.columns])\n",
    "    # Splitting the dataset into training, validation, and test sets\n",
    "    train_size = int(len(data)*0.8)\n",
    "    val_size =int(train_size*0.10)\n",
    "    #Train_val_test sets\n",
    "    Xy_train = data[:train_size-val_size]\n",
    "    Xy_val= data[train_size-val_size:train_size]\n",
    "    Xy_test=data[train_size:]\n",
    "    return Xy_train,Xy_val,Xy_test\n",
    "Xy_train,Xy_val,Xy_test=Make_tidydata(Abj_dt,scaler)\n",
    "def dates_splitting(date):\n",
    "    # Splitting the dataset into training, validation, and test sets\n",
    "    train_size = int(len(date)*0.8)\n",
    "    val_size =int(train_size*0.10)\n",
    "    #Train_val_test sets\n",
    "    date_train = date[:train_size-val_size]\n",
    "    date_val= date[train_size-val_size:train_size]\n",
    "    date_test=date[train_size:]\n",
    "    return date_train,date_val,date_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data \n",
    "X_train,y_train=Xy_train.values[:,:-1],Xy_train.values[:,-1]\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)  # Convert to float32\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)  # Convert to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Generate toy regression data\n",
    "# X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "# # Normalize features\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# y = y.reshape(-1, 1)  # Make y a column vector\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train = torch.tensor(X, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train.reshape(-1, 1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()  # Regression loss\n",
    "\n",
    "epochs = 100\n",
    "units=1\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        tkan_=TKAN(units, X_batch) \n",
    "        optimizer = torch.optim.Adam(tkan_.parameters(), lr=0.01) # This one should not  be inside the for loop, but the network defintion is problemeatic because of the X_batch\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "         # This \n",
    "        # tkan_( X_batch) # The input in this case should be sequences not X_batch\n",
    "        y_pred = tkan_(X_batch, update_grid=True)  # Forward pass\n",
    "        loss = criterion(y_pred, y_batch)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {epoch_loss / len(train_loader)}\") #:.4f\n",
    "\n",
    "print(\"Training complete! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
